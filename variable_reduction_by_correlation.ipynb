{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable/Feature Reduction Using Correlation in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we need to reduce features?\n",
    "\n",
    "When building a statistical model, it is quite likely that we have lots of variables or features, potentially from different sources. These could represent various different information about a particular observation. \n",
    "\n",
    "But in case we are using a large number of features, it is likely that some of these might be identical, some could be a scalar multiple (for example, height in inches in column x and height in cm in column y). Others could be linear or polynomial combinations of a few variables. Therefore, there are only a few \"dimensions\" that are representede as various features i.e. the various of the input features is largely represented by only a few features. \n",
    "\n",
    "In order for us to build a parsimonious model, we need to limit the number of \"dimensions\" or types of information. This could be done in various ways such as removing duplicate or scalar multiples of variables or more complex ways such as using Principal Component Analysis (PCA) or variable clustering (K-means, hierarchical, etc.). The latter methods are arguably better but relatively harder to interpret for the lay person and also computationally expensive (especially when you have 1000s of features).\n",
    "\n",
    "\n",
    "## Where does correlation come in?\n",
    "\n",
    "As I said earlier, there are less effective but efficient and easy to interpret ways and other statistically better but computationally expensive and difficult to interpret ways of feature reduction. How can we be smarter about this?\n",
    "\n",
    "1. Apply efficient techniques first using lower \"thresholds\". This would only remove features that are identical or very very similar to each other\n",
    "2. Then, apply statistically better techniques that would be able to more complex ways to remove \"dimensions\" and not just features. Since we are left with fewer variables (due to step 1), this should be much quicker\n",
    "\n",
    "Now step 1 is more critical the more redundancies there are in your data. If your data is clean and you know most features represent very different types of information, you could directly use step 2. Step 1 is where correlation based techniques come in.\n",
    "\n",
    "\n",
    "## Different methods to put correlation to work\n",
    "\n",
    "In this notebook, I've highlighted two main ways to put correlation to work. Please note that **only works with numeric data types**. \n",
    "1. Correlation with dependant variable (or target). This is the correlation of independent variables with the dependent variables\n",
    "2. Cross correlation. This is the correlation between variable pairs among the features (independant variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation with dependent variable\n",
    "\n",
    "This is the correlation of each of the input features with the target. There are different ways to calculate correlation and they are best explained <a href=\"https://www.kaggle.com/kiyoung1027/correlation-pearson-spearman-and-kendall\" target = \"_blank\">here</a>. Now, think of the following examples:\n",
    "1. Consider an equation of a line y = x where y is the target and x is the input feature. If that represents the data perfectly then correlation between x and y is 1\n",
    "2. Consider a parabola (like the U-shaped correlation as shown in the link), the correlation is 0 but that need not mean the variable is meaningless\n",
    "3. Now, consider a line y = 5 i.e. a variable containing the value 5 in all obsevations. This may hold a non-zero correlation but is meaningless to use in building a model\n",
    "\n",
    "From these examples, it appears that lower correlations need to be examined and if \"certain conditions\" (will explain in just a bit) are met, they can be excluded from the input features and dataset. How do we calculate the correlation with target? By using the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_target(df_input, df_target, corr_coeff=0.95, savefile=0):\n",
    "\t\"\"\" The function retuns a list of features to be dropped from the input features.\n",
    "\t\n",
    "\tINPUTS:\n",
    "\t1. df_input: n numeric input features (pandas dataframe)\n",
    "\t2. df_target: Target values (ensure same order as input features)\n",
    "\t3. corr_coeff: Coefficient threshold (absolute value, no negatives) for feature and target below which feature will be dropped\n",
    "\t4. savefile: If set to 1, all relevant files will be saved\n",
    "\t\n",
    "\tPLEASE NOTE:\n",
    "\t- The dataframe df_input should contain only the n numeric input features (i.e. no ID and targets) \n",
    "\t- The pandas series df_target should only be 1 column (if multiclass it should include all classes) and should be in the same order as the input dataset df_input\n",
    "\n",
    "\tSUMMARY OF LOGIC:\n",
    "\t1. The n numeric input variables are taken and a n-dimensional vector of correlation is created (these are absolute values i.e. a correlation of -0.8 is treated as 0.8)\n",
    "\t2. Variables with correlation lower than the corr_coeff threshold are dropped \n",
    "\n",
    "\tSAVED FILES:\n",
    "\tIf savefile is set to 1. Saved under current directory under corr_target under folder with a UTC timestamp.\n",
    "\t1. A CSV of the vector containing the values used for the heatmap\n",
    "\t2. A CSV of the list of variables to be dropped\n",
    "\t\"\"\"\n",
    "\t\n",
    "\t# Pre-processing\n",
    "\timport pandas as pd\n",
    "\tif savefile == 1:\n",
    "\t\tfrom datetime import datetime\n",
    "\t\ttime = str(datetime.utcnow())\n",
    "\t\timport os\n",
    "\t\tpath = str(os.getcwd()) + \"/corr_target/\" + str(time[0:19].replace(':',\"-\")) + \"/\"\n",
    "\t\tos.makedirs(path)\n",
    "\n",
    "\t# Combining the input and target data\n",
    "\tdf = pd.DataFrame(df_input)\n",
    "\tdf[\"target\"] = pd.Series(df_target)\n",
    "\n",
    "\t# Generating correlation matrix of input features\n",
    "\tcorr_matrix = df.corr(method = 'pearson') # For more info on the methods please refer to https://www.kaggle.com/kiyoung1027/correlation-pearson-spearman-and-kendall\n",
    "\tcorr_matrix = corr_matrix.iloc[0:corr_matrix.shape[0]-1,corr_matrix.shape[0]-1]\n",
    "\n",
    "\t# Saving files\n",
    "\tif savefile ==1:\n",
    "\t\tcorr_matrix.to_csv(path + \"cross_corr.csv\")\n",
    "\n",
    "\n",
    "\t# Selecting features to be dropped (Using two for loops that runs on one triangle of the corr_matrix to avoid checking the correlation of a variable with itself)\n",
    "\tcorr_matrix = abs(corr_matrix)\n",
    "\tfeatures_drop_list = list(corr_matrix[corr_matrix<corr_coeff].index)\n",
    "\n",
    "\t# Saving final list\n",
    "\tif savefile ==1:\n",
    "\t\tpd.Series(features_drop_list).to_csv(path + \"features_drop_list.csv\")\n",
    "\n",
    "\treturn features_drop_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I typically use thresholds less than 1% (i.e. 0.01) but at most 2%. Now DO NOT directly drop all these features. Once the list is generated try plotting univariates (variable vs target of each variable to be dropped) and bivariates (same as univariates but with population or frequency added) to check if any of these need to be retained. Drop these variables and now proceed to check cross-correlation.\n",
    "\n",
    "### Cross-correlation\n",
    "\n",
    "The above function is used to elimate features based on how they independently correlate with the dependent variable. But what happens if there is are features that are scalar multiples of each other (like height in cm and inches) that are highly correlated with the dependent variable? We can remove one of such pairs using the correlation between pairs of variables. If there are n features, then we need to calculate an n X n where each cell is the correlation between the pairs of variables corresponding to the row and column.\n",
    "\n",
    "Suppose we see that a 2 variables are highly correlated, which one do we eliminate? There are two ways of doing this:\n",
    "1. Eliminate the variable which has lower correlation with target\n",
    "2. Eliminate the variable which has a higher mean correlation with all the input variables in the dataset\n",
    "\n",
    "How do these methods differ? Option 1 uses the correlation technique we discussed previously to eliminate one of the two. But like we discussed earlier, a lower correlation might not necessarily mean we need to drop the variable (Think of that parabola shape we discussed earlier). A lower mean correlation with all variables doesn't check with the target but with other variables. A similar condition which could imply a similar drawback. \n",
    "\n",
    "I personally prefer option 2 as in my personal experience, checking with multiple variables usually results in less cases of parabolas and more cases of straightforward correlations but you need to explore your data and figure it out. Or, you could reduce the threshold and try both and use a superset of what you get. But since this is the quick and dirty variable reduction being performed, we need not aim for the absolute best method. That can be handled by variable clustering, PCA, etc.\n",
    "\n",
    "### Cross-correlation (dropping by correlation with target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_corr_target(df_input, df_target, corr_coeff=0.95, plot=0, savefile=0):\n",
    "\t\"\"\" The function retuns a list of features to be dropped from the input features.\n",
    "\t\n",
    "\tINPUTS:\n",
    "\t1. df_input: n numeric input features (pandas dataframe)\n",
    "\t2. df_target: Target values (ensure same order as input features)\n",
    "\t3. corr_coeff: Coefficient threshold (absolute value, no negatives) for a pair of variables above which one of the two will be dropped\n",
    "\t4. plot: If set to 1 a plot will be displayed showing a heatmap of the cross-correlation between variables \n",
    "\t5. savefile: If set to 1, all relevant files will be saved\n",
    "\t\n",
    "\tPLEASE NOTE:\n",
    "\t- The dataframe df_input should contain only the n numeric input features i.e. no ID and targets) \n",
    "\t- The pandas series df_target should only be 1 column (if multiclass it should include all classes) and should be in the same order as the input dataset df_input\n",
    "\n",
    "\tSUMMARY OF LOGIC:\n",
    "\t1. The n numeric input variables are taken and a n X n matrix of correlation is created (these are absolute values i.e. a correlation of -0.8 is treated as 0.8)\n",
    "\t2. Variable pairs with correlation higher than the corr_coeff threshold are picked and one of the two variables will be dropped\n",
    "\t3. Which of the two will be dropped is based on the one having lower correlation with target variable\n",
    "\n",
    "\tSAVED FILES:\n",
    "\tIf savefile is set to 1. Saved under current directory under cross_corr_target under folder with a UTC timestamp.\n",
    "\t1. A PDF heatmap representing the cross correlation between all the input features\n",
    "\t2. A CSV of the matrix containing the values used for the heatmap\n",
    "\t3. A CSV of correlation of input features with the target\n",
    "\t4. A CSV of the list of variables to be dropped\n",
    "\t\"\"\"\n",
    "\t\n",
    "\t# Pre-processing\n",
    "\timport pandas as pd\n",
    "\tif savefile == 1:\n",
    "\t\tfrom datetime import datetime\n",
    "\t\ttime = str(datetime.utcnow())\n",
    "\t\timport os\n",
    "\t\tpath = str(os.getcwd()) + \"/cross_corr_target/\" + str(time[0:19].replace(':',\"-\")) + \"/\"\n",
    "\t\tos.makedirs(path)\n",
    "\n",
    "\t# Combining the input and target data\n",
    "\tdf = pd.DataFrame(df_input)\n",
    "\tdf[\"target\"] = pd.Series(df_target)\n",
    "\n",
    "\t# Generating correlation matrix of input features\n",
    "\tcorr_matrix = df.corr(method = 'pearson') # For more info on the methods please refer to https://www.kaggle.com/kiyoung1027/correlation-pearson-spearman-and-kendall\n",
    "\n",
    "\t# Plotting cross correlation matrix\n",
    "\tif plot == 1:\n",
    "\t\timport matplotlib.pyplot as plt\n",
    "\t\timport seaborn as sns\n",
    "\t\tplt.figure(figsize=(10,8))\n",
    "\t\tsns.heatmap(corr_matrix.iloc[0:corr_matrix.shape[0]-1,0:corr_matrix.shape[0]-1].round(2), cmap=plt.cm.Blues)\n",
    "\t\tfig = plt.gcf()\n",
    "\t\tplt.show()\n",
    "\t\tif savefile == 1:\n",
    "\t\t\tfig.savefig(path + \"cross_corr_heatmap.pdf\")\n",
    "\n",
    "\t# Generating correlation with the target\n",
    "\tcorr_target = (corr_matrix[\"target\"])\n",
    "\n",
    "\t# Saving files\n",
    "\tif savefile ==1:\n",
    "\t\tcorr_matrix = corr_matrix.iloc[0:corr_matrix.shape[0]-1,0:corr_matrix.shape[0]-1] # Removing the last row and column which contain the target\n",
    "\t\tcorr_target = corr_target.iloc[0:corr_target.shape[0]-1] # Removing the value which contain the target\n",
    "\t\tcorr_matrix.to_csv(path + \"cross_corr.csv\")\n",
    "\t\tcorr_target.to_csv(path + \"target_corr.csv\")\n",
    "\n",
    "\t# Preparing data\n",
    "\tfeatures_drop_list = [] # This will contain the list of features to be dropped\n",
    "\tfeatures_index_drop_list = [] # This will contain the index of features to be dropped as per df_input\n",
    "\tcorr_matrix = abs(corr_matrix)\n",
    "\tcorr_target = abs(corr_target)\n",
    "\n",
    "\t# Selecting features to be dropped (Using two for loops that runs on one triangle of the corr_matrix to avoid checking the correlation of a variable with itself)\n",
    "\tfor i in range(corr_matrix.shape[0]):\n",
    "\t\tfor j in range(i+1,corr_matrix.shape[0]):\n",
    "\n",
    "\t\t\t# The following if statement checks if each correlation value is higher than threshold (or equal) and also ensures the two columns have NOT been dropped already.  \n",
    "\t\t\tif corr_matrix.iloc[i,j]>=corr_coeff and i not in features_index_drop_list and j not in features_index_drop_list:\n",
    "\t\t\t\n",
    "\t\t\t\t# The following if statement checks which of the 2 variables with high correlation has a lower correlation with target and then drops it. If equal we can drop any and it drops the first one (This is arbitrary)\n",
    "\t\t\t\tif corr_target[corr_matrix.columns[i]] >= corr_target[corr_matrix.columns[j]]:\n",
    "\t\t\t\t\tfeatures_drop_list.append(corr_matrix.columns[j])\t# Name of variable that needs to be dropped appended to list\n",
    "\t\t\t\t\tfeatures_index_drop_list.append(j)\t# Index of variable that needs to be dropped appended to list. This is used to not check for the same variables repeatedly\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tfeatures_drop_list.append(corr_matrix.columns[i])\n",
    "\t\t\t\t\tfeatures_index_drop_list.append(i)\n",
    "\t\n",
    "\t# Saving final list\n",
    "\tif savefile ==1:\n",
    "\t\tpd.Series(features_drop_list).to_csv(path + \"features_drop_list.csv\")\n",
    "\n",
    "\treturn features_drop_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-correlation (dropping by mean correlation with all features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_corr_mean(df_input, corr_coeff=0.95, plot=0, savefile=0):\n",
    "\t\"\"\" The function retuns a list of features to be dropped from the input features.\n",
    "\t\n",
    "\tINPUTS:\n",
    "\t1. df_input: n input features (pandas dataframe)\n",
    "\t2. corr_coeff: Coefficient threshold (absolute value, no negatives) for a pair of variables above which one of the two will be dropped\n",
    "\t3. plot: If set to 1 a plot will be displayed showing a heatmap of the cross-correlation between variables \n",
    "\t4. savefile: If set to 1, all relevant files will be saved\n",
    "\t\n",
    "\tPLEASE NOTE:\n",
    "\t- The dataframe df_input (should contain only the n input features i.e. no ID and targets) \n",
    "\t\n",
    "\tSUMMARY OF LOGIC:\n",
    "\t1. The n input variables are taken and a n X n matrix of correlation is created (these are absolute values i.e. a correlation of -0.8 is treated as 0.8)\n",
    "\t2. Variable pairs with correlation higher than the corr_coeff threshold are picked and one of the two variables will be dropped\n",
    "\t3. Which of the two will be dropped is based on the one having lower mean absolute correlation with all other variables \n",
    "\n",
    "\tSAVED FILES:\n",
    "\tIf savefile is set to 1. Saved under current directory under cross_corr_target under folder with a UTC timestamp.\n",
    "\t1. A PDF heatmap representing the cross correlation between all the input features\n",
    "\t2. A CSV of the matrix containing the values used for the heatmap\n",
    "\t3. A CSV of the list of variables to be dropped\n",
    "\t\"\"\"\n",
    "\t\n",
    "\t# Pre-processing\n",
    "\timport pandas as pd\n",
    "\tif savefile == 1:\n",
    "\t\tfrom datetime import datetime\n",
    "\t\ttime = str(datetime.utcnow())\n",
    "\t\timport os\n",
    "\t\tpath = str(os.getcwd()) + \"/cross_corr_mean/\" + str(time[0:19].replace(':',\"-\")) + \"/\"\n",
    "\t\tos.makedirs(path)\n",
    "\n",
    "\t# Generating correlation matrix of input features\n",
    "\tcorr_matrix = df_input.corr(method = 'pearson') # For more info on the methods please refer to https://www.kaggle.com/kiyoung1027/correlation-pearson-spearman-and-kendall\n",
    "\n",
    "\t# Plotting cross correlation matrix\n",
    "\tif plot == 1:\n",
    "\t\timport matplotlib.pyplot as plt\n",
    "\t\timport seaborn as sns\n",
    "\t\tplt.figure(figsize=(10,8))\n",
    "\t\tsns.heatmap(corr_matrix.round(2), cmap=plt.cm.Blues)\n",
    "\t\tfig = plt.gcf()\n",
    "\t\tplt.show()\n",
    "\t\tif savefile == 1:\n",
    "\t\t\tfig.savefig(path + \"cross_corr_heatmap.pdf\")\n",
    "\n",
    "\t# Generating correlation with the target\n",
    "\tcorr_mean = abs(corr_matrix).mean()\n",
    "\n",
    "\t# Saving files\n",
    "\tif savefile ==1:\n",
    "\t\tcorr_matrix.to_csv(path + \"cross_corr.csv\")\n",
    "\t\tcorr_mean.to_csv(path + \"corr_abs_mean.csv\")\n",
    "\n",
    "\t# Preparing data\n",
    "\tfeatures_drop_list = [] # This will contain the list of features to be dropped\n",
    "\tfeatures_index_drop_list = [] # This will contain the index of features to be dropped as per df_input\n",
    "\tcorr_matrix = abs(corr_matrix)\n",
    "\n",
    "\t# Selecting features to be dropped (Using two for loops that runs on one triangle of the corr_matrix to avoid checking the correlation of a variable with itself)\n",
    "\tfor i in range(corr_matrix.shape[0]):\n",
    "\t\tfor j in range(i+1,corr_matrix.shape[0]):\n",
    "\n",
    "\t\t\t# The following if statement checks if each correlation value is higher than threshold (or equal) and also ensures the two columns have NOT been dropped already.  \n",
    "\t\t\tif corr_matrix.iloc[i,j]>=corr_coeff and i not in features_index_drop_list and j not in features_index_drop_list:\n",
    "\t\t\t\n",
    "\t\t\t\t# The following if statement checks which of the 2 variables with high correlation has a lower correlation with target and then drops it. If equal we can drop any and it drops the first one (This is arbitrary)\n",
    "\t\t\t\tif corr_mean[corr_matrix.columns[i]] >= corr_mean[corr_matrix.columns[j]]:\n",
    "\t\t\t\t\tfeatures_drop_list.append(corr_matrix.columns[i])\t# Name of variable that needs to be dropped appended to list\n",
    "\t\t\t\t\tfeatures_index_drop_list.append(i)\t# Index of variable that needs to be dropped appended to list. This is used to not check for the same variables repeatedly\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tfeatures_drop_list.append(corr_matrix.columns[j])\n",
    "\t\t\t\t\tfeatures_index_drop_list.append(j)\n",
    "\t\n",
    "\t# Saving final list\n",
    "\tif savefile ==1:\n",
    "\t\tpd.Series(features_drop_list).to_csv(path + \"features_drop_list.csv\")\n",
    "\n",
    "\treturn features_drop_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please note\n",
    "\n",
    "1. The input features accepted here are only numeric in nature. Characters, strings, booleans, etc. do not work. Convert these to numeric types before use. Or use other techniques like Cramer's V\n",
    "2. The target data to be input needs to be one column (even if multiclass) and needs to be in the same order as the observations in the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
